{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b813a7d1",
   "metadata": {},
   "source": [
    "# Indeed Webscraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7fdf6c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9acb191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module provides various time-related functions.\n",
    "import time\n",
    "# The datetime module supplies classes for manipulating dates and times.\n",
    "import datetime\n",
    "# API to access Selenium WebDrivers like Firefox, Ie, Chrome\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# os gives access to the operating system\n",
    "import os\n",
    "# CSV File Reading and WritingÂ¶\n",
    "import csv\n",
    "# standard python library for logging\n",
    "import logging\n",
    "# Pandas is an open source data analysis and manipulation tool\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314496eb",
   "metadata": {},
   "source": [
    "## Indeed Job Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc37b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time until next command is executed after a new window loaded\n",
    "loadWindow_Time = 70\n",
    "# time between clicking different jobs\n",
    "sleepTime_betweenJobs = 100\n",
    "# ammount of attempts the software needs to execute before going to next step\n",
    "attempts = 3\n",
    "\n",
    "# function that starts Google Chrome\n",
    "def startChromeEngine():\n",
    "    # Website that will get scraped\n",
    "    Indeed_Url = \"https://de.indeed.com/?r=us\"\n",
    "    # Path to webdriver\n",
    "    ser = Service(\"/Applications/chromedriver\")\n",
    "\n",
    "    # start chrome driver\n",
    "    driver = webdriver.Chrome(service=ser)\n",
    "    # load Indeed URL\n",
    "    driver.get(Indeed_Url)\n",
    "\n",
    "    # wait for cookies window\n",
    "    time.sleep(loadWindow_Time)\n",
    "    \n",
    "    #return driver when window is loaded \n",
    "    return driver\n",
    "\n",
    "# function that starts the search engine on Indeed.com\n",
    "def startSearchEngine(driver, job_title, location):\n",
    "    # accept cookies\n",
    "    driver.find_element(By.XPATH, \"//*[@id=\\\"onetrust-accept-btn-handler\\\"]\").click()\n",
    "\n",
    "    # enter job title\n",
    "    driver.find_element(By.XPATH, \"//*[@id=\\\"text-input-what\\\"]\").send_keys(job_title)\n",
    "    # enter location\n",
    "    driver.find_element(By.XPATH, \"//*[@id=\\\"text-input-where\\\"]\").send_keys(location)\n",
    "    # start search\n",
    "    driver.find_element(By.XPATH, \"//*[@id=\\\"text-input-where\\\"]\").send_keys(Keys.RETURN)\n",
    "\n",
    "    # wait for page to load \n",
    "    time.sleep(loadWindow_Time)\n",
    "    \n",
    "    # sort result by publish data\n",
    "    driver.find_element(By.XPATH, \"//*[@id=\\\"resultsCol\\\"]/div[3]/div[4]/div[1]/span[2]/a\").click()\n",
    "    \n",
    "    # wait for random pop up window\n",
    "    time.sleep(loadWindow_Time)    \n",
    "    # close random pop up window\n",
    "    driver.find_element(By.XPATH,\"//*[@id=\\\"popover-x\\\"]/button\").click()\n",
    "    \n",
    "    # wait for page to load\n",
    "    time.sleep(loadWindow_Time)\n",
    "\n",
    "    return driver\n",
    "\n",
    "# function that scrapes every job page \n",
    "def startScrapingPage(driver, all_jobs_lst):\n",
    "    # iterating over attempts because sometimes the container doesnt get found immediately\n",
    "    for attempt in range(attempts):\n",
    "        # try to find the container where all jobs are presented\n",
    "        try:\n",
    "            job_list_container = driver.find_element(By.XPATH, \"//*[@id=\\\"mosaic-provider-jobcards\\\"]\")\n",
    "        #try again after 10 seconds...\n",
    "        except:\n",
    "            time.sleep(10)\n",
    "\n",
    "            \n",
    "    # get all jobs in container\n",
    "    job_list = job_list_container.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "    # get all job ids in job_list\n",
    "    job_ids = []\n",
    "    try:\n",
    "        # iterate over every job in job_list\n",
    "        for job in job_list:\n",
    "            # get the job id to later click on every job...\n",
    "            id = job.get_attribute(\"id\")\n",
    "            # if job id is not empty add do job_ids list\n",
    "            if id != \"\":\n",
    "                job_ids.append(id)\n",
    "    except:\n",
    "        # sleep time at this point is maybe overkill but i keep it in to help against reCaptcha \n",
    "        time.sleep(loadWindow_Time)\n",
    "\n",
    "    # iterate over every job in job_ids\n",
    "    for job_id in job_ids:\n",
    "        try:\n",
    "            # try to click on current job_id in order to see full job description\n",
    "            driver.find_element(By.XPATH, f\"//*[@id=\\\"{job_id}\\\"]\").click()\n",
    "            # wait until description is loaded\n",
    "            time.sleep(loadWindow_Time)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # attempt multiple times because its not always working immediately\n",
    "        for attempt in range (attempts):\n",
    "            # try to switch to Job Description iframe in order to be able to interact with it\n",
    "            try:\n",
    "                # find iframe\n",
    "                iframe = driver.find_element(By.XPATH,\"//*[@id=\\\"vjs-container-iframe\\\"]\")\n",
    "                # switch to iframe\n",
    "                driver.switch_to.frame(iframe)\n",
    "                break\n",
    "            except:\n",
    "                # try again \n",
    "                time.sleep(loadWindow_Time)\n",
    "                \n",
    "        # attempt multiple times because its not always working immediately\n",
    "        for attempt in range(attempts):\n",
    "            # get job top card\n",
    "            try:\n",
    "                topCard = driver.find_element(By.XPATH, \"//*[@id=\\\"viewJobSSRRoot\\\"]/div/div[1]/div/div/div/div[1]/div/div[1]\")\n",
    "            except:\n",
    "                # if we cant find the jobcard we wont find the below informations...\n",
    "                job_title = \"\"\n",
    "                company_name = \"\"\n",
    "                company_location = \"\"\n",
    "                job_type = \"\"\n",
    "                time.sleep(loadWindow_Time)\n",
    "                continue\n",
    "\n",
    "            # get job title\n",
    "            try:\n",
    "                job_title = topCard.find_element(By.CSS_SELECTOR,\".icl-u-xs-mb--xs.icl-u-xs-mt--none.jobsearch-JobInfoHeader-title.is-embedded\").text\n",
    "            except:\n",
    "                job_title = \"\"\n",
    "\n",
    "            # get company name\n",
    "            try:\n",
    "                company_name = driver.find_element(By.CSS_SELECTOR, \".icl-u-lg-mr--sm.icl-u-xs-mr--xs\").text\n",
    "            except:\n",
    "                company_name = \"\"\n",
    "\n",
    "            # get company location\n",
    "            try:\n",
    "                company_location = driver.find_element(By.CSS_SELECTOR,\"div[class='jobsearch-CompanyInfoWithoutHeaderImage'] div:nth-child(2)\").text\n",
    "            except:\n",
    "                company_location = \"\"\n",
    "            # get type of job\n",
    "            try:\n",
    "                job_type = driver.find_element(By.CSS_SELECTOR, \"body > div:nth-child(4) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > div:nth-child(2) > span:nth-child(1)\").text\n",
    "            except:\n",
    "                job_type = \"\"\n",
    "\n",
    "\n",
    "\n",
    "        # attempt multiple times because its not always working immediately\n",
    "        for attempt in range(attempts):\n",
    "            # try to get full job description\n",
    "            try:\n",
    "                job_Description = driver.find_element(By.XPATH, \"//*[@id=\\\"jobDescriptionText\\\"]\").text\n",
    "                break\n",
    "            except:\n",
    "                job_Description = \"\"\n",
    "                time.sleep(loadWindow_Time)\n",
    "\n",
    "        # get current datetime\n",
    "        date_time = datetime.datetime.now().strftime(\"%d%b%Y-%H:%M:%S\")\n",
    "\n",
    "        # add all_jobs to all_jobs_lst\n",
    "        job = [job_id, job_title, company_name, company_location, job_type, job_Description, date_time]\n",
    "        all_jobs_lst.append(job)\n",
    "\n",
    "        # switch back to default frame to click on next job\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "    return all_jobs_lst\n",
    "\n",
    "# function that clicks on next page\n",
    "def getNextPage(driver, page_Number):\n",
    "    # click on next Page button\n",
    "    # xpath changes after first page\n",
    "    if page_Number ==0:\n",
    "        driver.find_element(By.XPATH,\"//*[@id=\\\"resultsCol\\\"]/nav/div/ul/li[6]/a\").click()\n",
    "    else:\n",
    "        driver.find_element(By.XPATH,\"//*[@id=\\\"resultsCol\\\"]/nav/div/ul/li[7]/a\").click()\n",
    "    #wait until page is loaded\n",
    "    time.sleep(loadWindow_Time)\n",
    "\n",
    "    return driver\n",
    "\n",
    "# function that saves jobs as csv \n",
    "def saveAsCSV(all_jobs_lst, page_nr):\n",
    "    dateTime=datetime.datetime.now()    \n",
    "    filename_dateTime = dateTime.strftime(\"%d%m%Y_%H_%M_%S\")\n",
    "    df = pd.DataFrame(all_jobs_lst)\n",
    "    df.to_csv(\"/Users/jan/Documents/7.Semester/Datenanalyse in der Praxis/SeminarArbeit/Data/Indeed_Jobs\" + str(page_nr) + str(filename_dateTime)  + \".csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb02b442",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "869e5588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "getNextPage() missing 1 required positional argument: 'page_Number'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c64fcef00235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msaveAsCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_jobs_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_Number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#get next page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetNextPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: getNextPage() missing 1 required positional argument: 'page_Number'"
     ]
    }
   ],
   "source": [
    "# max number of pages we try to scrape, usually ReCaptcha catches us before we get there...\n",
    "ammount_Of_Pages=35\n",
    "# start chrome engine\n",
    "driver = startChromeEngine()\n",
    "# start the search\n",
    "driver = startSearchEngine(driver, \"Data Analyst\", \"Deutschland\")\n",
    "# iterate over pages\n",
    "for page_Number in range(ammount_Of_Pages):\n",
    "    # initialize job list\n",
    "    all_jobs_lst=[]\n",
    "    # start scraping current page\n",
    "    all_jobs_lst = startScrapingPage(driver,all_jobs_lst)\n",
    "    #save as csv\n",
    "    saveAsCSV(all_jobs_lst, page_Number)\n",
    "    #get next page\n",
    "    driver = getNextPage(driver,page_Number)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d5bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
